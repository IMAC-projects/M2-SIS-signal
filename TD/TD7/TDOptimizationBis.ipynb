{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this TD, we aim at exploring the gradient descent method for optimizing smooth functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy.signal as sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Gradient descent in 2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.1, 1])\n",
    "f = lambda x : return np.sum(weights * x**2, axis=-1)\n",
    "gradf = lambda x :weights * 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFunction(funct, minn, maxx, definition = 200, title = 'Title'):\n",
    "  xx, yy = np.meshgrid(np.linspace(minn[0], maxx[0], definition), np.linspace(minn[1], maxx[1], definition))\n",
    "  zz = funct(np.stack((xx, yy), axis=-1))\n",
    "  axes = plt.gca()\n",
    "  axes.set_ylim([minn[0], maxx[0]])\n",
    "  axes.set_xlim([minn[1], maxx[1]])\n",
    "  plt.contourf(xx, yy, zz, definition, cmap=plt.cm.rainbow)\n",
    "  plt.axhline(0, color='black')\n",
    "  plt.axvline(0, color='black')\n",
    "  plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "drawFunction(f, [-1, -1], [1, 1], title='function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex: Implement a generic gradient descent algorithm taking an initial point, the gradient of the function, a number of steps and a stepsize $\\varepsilon$.   Compute by hand  the Lipschitz constant of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps < 2/L\n",
    "# where L = 2* max weights = 2 \n",
    "\n",
    "def GradientDescent(x0, gradient, iterations=50, eps = 0.01):\n",
    "    xValues = [x0]\n",
    "    for _ in range(iterations-1):\n",
    "        last = xValues[len(xValues)-1]\n",
    "        xValues.append(last - eps * gradient(last))\n",
    "    return xValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex:  Plot the gradient descent for different initializations and different step size of the gradient descent. Try to show that the convergence can be really slow when the ratio between the two weights is very large or very small. This ratio is often called the condition number. Other optimization algorithms have to be used in order to avoid this slow convergence such as quasi-Newton methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ptsNb = 10\n",
    "pts = np.random.rand(ptsNb, 2)*2-1\n",
    "\n",
    "gradientPts = np.array(GradientDescent(pts, gradf, iterations=50, eps=0.05))\n",
    "\n",
    "gradientPts = np.rollaxis(gradientPts, 1)\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(gradientPts)))\n",
    "\n",
    "drawFunction(f, [-1, -1], [1, 1], title='function')\n",
    "\n",
    "for x, c in zip(gradientPts, colors):\n",
    "    plt.scatter(x[:, 0], x[:, 1], color=c, marker='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Example on a physical system\n",
    "\n",
    "### We consider a massive ball of mass $m$, subject to gravity and the force of the wind. The ball is at rest at time $0$ in position $x_0 \\in \\mathbb{R}^3$. We want to find the initial $v0$ such that the ball will land at a given final point $x_1$ at time $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement the equations of the motion of the ball.\n",
    "def Wind(t):\n",
    "    return -np.asarray([-2,3,-0.2]) * np.cos(3*t) \n",
    "\n",
    "def GetPositionAtTime1(x0,v0,wind):\n",
    "    number_of_timesteps = 1000\n",
    "    timestep = 1.0/number_of_timesteps\n",
    "    temp_x = x0.copy()\n",
    "    temp_v = v0.copy()\n",
    "    time = 0\n",
    "    trajectory = [x0]\n",
    "    times = [0]\n",
    "    ## note that we use here a naive integrator: forward Euler.\n",
    "    for i in range(number_of_timesteps):\n",
    "        temp_x += timestep * (temp_v)\n",
    "        temp_v += timestep * (2*np.asarray([0, 0, -1]) + wind(time)) #2 is a chosen value representing the gravity times the mass\n",
    "        time += timestep\n",
    "        trajectory.append(temp_x.copy())\n",
    "        times.append(time)\n",
    "    return temp_x,trajectory,times     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We test the computation of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "x0 = np.asarray([0.,0.,0.]) # intial position\n",
    "v0 = np.asarray([0.,0.,0.]) # initial speed\n",
    "x1,trajectory,times = GetPositionAtTime1(x0,v0,Wind)\n",
    "print(\"position at time 1\",x1)\n",
    "data = np.asarray(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We plot the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the trajectory\n",
    "data = np.asarray(trajectory)\n",
    "## Plot of the trajectory\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "mpl.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot(data[:,0], data[:,1], data[:,2], label='trajectory')\n",
    "ax.legend()\n",
    "ax.scatter(x0[0],x0[1],x0[2],\"bo\",linewidths = 20)\n",
    "ax.scatter(x1[0],x1[1],x1[2],\"ro\",linewidths = 20)\n",
    "plt.show()\n",
    "print(x0,x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex: Implement the following approximation of the gradient: $\\partial_i f(x) \\simeq \\frac{f(x + \\varepsilon e_i) - f(x)}{\\varepsilon}$ with $e_i$ is the vector $(0,\\ldots,1, \\ldots , 0)$ where the $1$ is at the $i^{\\text{th}}$ position. Then, recall that the gradient at point $x$ of the function $f$ is the vector $(\\partial_i f(x))_{i \\in 1,\\ldots,n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here.\n",
    "def ApproximatedGradient(myFunction,x):\n",
    "    return gradientAtx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex: Using the previous approximation of the gradient, use a gradient descent with constant step size to solve the problem. For that, we define the $L^2$ penalty function between the target and the point position at time $1$ and we compute the gradient. Use it to solve the problem and plot the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2Penalty(finalPosition,target):\n",
    "    return np.sum((finalPosition - target)**2)\n",
    "\n",
    "def ComposedGradient(finalPosition,gradientFinalPosition,target):\n",
    "    return 2*(finalPosition - t) * gradientFinalPosition\n",
    "\n",
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Example on a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as imio\n",
    "colored_image = imio.imread('lena.png')\n",
    "lena = np.sum(colored_image*[ 0.21, 0.72 ,0.07,0.0],axis=-1)\n",
    "sub_defense = defense[::2,::2]\n",
    "plt.gray()\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"lena\")\n",
    "plt.imshow(defense)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"lena sub-sampled\")\n",
    "plt.imshow(sub_defense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateMask(shape,threshold = 2.0):\n",
    "    temp = (np.random.rand(shape[0],shape[1])<0.5)*1.0\n",
    "    filtering = np.array([[1,1],[1,1]])\n",
    "    temp=  (sig.convolve(filtering,temp)<threshold)*1.0\n",
    "    return temp[:shape[0],:shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = GenerateMask(np.shape(sub_defense),2.0)\n",
    "print(\"averaged pixel value on the remaining pixels\",np.sum(sub_defense*mask)/np.sum(mask))\n",
    "print(\"percentage of remaining pixels\",np.sum(mask)/np.size(mask))\n",
    "\n",
    "plt.gray()\n",
    "taille = 20\n",
    "plt.figure(figsize = (taille,taille))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"lena\",size = taille)\n",
    "plt.imshow(mask);\n",
    "plt.subplot(1,2,2);\n",
    "plt.title(\"lena masked\",size = taille)\n",
    "plt.imshow(sub_defense*mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Compute on a sheet of paper the gradient of the function $f: \\mathbb{R}^d \\mapsto \\mathbb{R}$ defined by $f(x_1,\\ldots,x_n) = \\sum_{i = 1}^{n-1} \\frac12| x_i - x_{i+1}|^2$. \n",
    "* ### Compute the Lipschitz constant of the gradient (remark that it is a linear function).\n",
    "## We propose to reconstruct a candidate for our initial image as the minimizer of an energy functional $E(I) = \\sum_{i = 1}^{\\text{nbre lignes}} f(I[i,:])$ under the constraint that $I (1-\\text{mask}) = \\text{data}$.\n",
    "* ### Copy your gradient descent code and modify it to implement the projected gradient descent algorithm.\n",
    "* ### Apply the algorithm to the data and comment. How do you suggest to modify the energy functional in order to improve the results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: your answer here. $##############$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}